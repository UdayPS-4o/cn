# Hashing

Hashing is a technique used to map keys to values in a data structure, typically a hash table. It allows for efficient insertion, deletion, and retrieval of data.

## Hash Function

A **hash function** is a function that takes an input (or 'key') and returns a fixed-size string of bytes, typically a hash value or hash code. The hash value is then used to determine the position (index or slot) where the corresponding data should be stored in a hash table.

**Properties of a good hash function:**
* **Deterministic:** For a given input, the hash function must always produce the same output.
* **Uniform Distribution:** It should distribute keys as evenly as possible across the hash table to minimize collisions.
* **Efficiency:** It should be computationally fast to compute the hash value.
* **Avalanche Effect:** A small change in the input key should result in a significantly different hash value.
* **Minimize Collisions:** While difficult to avoid entirely, a good hash function aims to reduce the likelihood of different keys producing the same hash value.

## Address Calculation Technique

The address calculation technique refers to the process of using the hash value generated by the hash function to determine the specific slot or bucket in the hash table where the data element will be stored or retrieved.

Typically, if `h(key)` is the hash value and `m` is the size of the hash table, the address `index` is calculated as:

`index = h(key) % m`

This modulo operation ensures that the calculated index falls within the bounds of the hash table (0 to m-1).

## Common Hashing Functions

Several common methods are used to design hash functions:

1.  **Division Method:**
    * **Formula:** `h(key) = key % m`
    * **Explanation:** The key is divided by the size of the hash table (`m`), and the remainder is taken as the hash value.
    * **Choosing `m`:** It's often recommended that `m` be a prime number not too close to a power of 2. This helps in distributing the keys more uniformly.
    * **Advantages:** Simple and fast.
    * **Disadvantages:** If `m` is not chosen carefully (e.g., a power of 2), it can lead to poor distribution of keys if the keys have certain patterns.

2.  **Mid-Square Method:**
    * **Explanation:** The key is squared. Then, some digits from the middle of the squared result are selected as the hash value. The number of digits selected usually depends on the desired size of the hash table.
    * **Example:** If `key = 1234` and we need a 3-digit hash.
        * `key^2 = 1234^2 = 1522756`
        * Select middle 3 digits (e.g., 227): `h(1234) = 227`.
    * **Advantages:** Tends to produce a good distribution, as all digits of the key contribute to the result.
    * **Disadvantages:** Squaring can be computationally more intensive than the division method. The choice of "middle" digits needs to be consistent.

3.  **Folding Method:**
    * **Explanation:** The key is divided into several parts (chunks). These parts are then combined (folded) together using arithmetic operations (like addition) to form the hash value.
    * **Types:**
        * **Fold-Shifting:** The parts are added together.
        * **Fold-Boundary:** The alternating parts are reversed before addition.
    * **Example (Fold-Shifting):** `key = 12345678`, table size requires 3 digits.
        * Divide into parts: `123`, `456`, `78` (pad if necessary, e.g., `078`).
        * Add parts: `123 + 456 + 078 = 657`. So, `h(12345678) = 657`.
    * **Advantages:** Good for keys with many digits. Ensures all parts of the key influence the hash.
    * **Disadvantages:** Can be slightly more complex to implement than the division method.

4.  **String Hashing (Length-Based or Character-Based):**
    * **Explanation:** For string keys, various methods can be used:
        * **Sum of ASCII/Unicode values:** Add the ASCII (or Unicode) values of the characters in the string, then take modulo `m`.
            * `h("cat") = (ASCII('c') + ASCII('a') + ASCII('t')) % m`
        * **Polynomial Hashing:** Each character is treated as a coefficient in a polynomial, evaluated at a certain point.
            * `h("cat") = (ASCII('c') * p^2 + ASCII('a') * p^1 + ASCII('t') * p^0) % m`, where `p` is a prime number (e.g., 31, 37, 53). This is a widely used and effective method for strings.
    * **Advantages:** Specifically designed for string data. Polynomial hashing is generally very effective.
    * **Disadvantages:** Simple summation can lead to many collisions for anagrams (e.g., "cat" and "act" might hash to the same value if order isn't considered well).

## Collision

A **collision** occurs when two or more different keys produce the same hash value (i.e., they map to the same slot in the hash table). Since hash functions map a potentially large set of keys to a smaller set of indices, collisions are inevitable, especially as the hash table fills up.

## Collision Resolution Techniques

Methods used to handle collisions and ensure that all keys can be stored in the hash table despite sharing hash indices.

### 1. Open Addressing (Closed Hashing)

In open addressing, all elements are stored directly within the hash table itself. When a collision occurs, we probe or search for an alternative empty slot in the table.

####    a. Linear Probing
    * **Concept:** If a collision occurs at index `h(key)`, we try the next slot `(h(key) + 1) % m`. If that is also occupied, we try `(h(key) + 2) % m`, and so on, until an empty slot is found.
    * **Probe Sequence:** `hash(key), (hash(key) + 1) % m, (hash(key) + 2) % m, ...`
    * **Advantages:**
        * Simple to implement.
        * Good cache performance due to locality of reference.
    * **Disadvantages:**
        * **Primary Clustering:** Tendency for occupied slots to form long sequences (clusters). As clusters grow, the average search time increases because we might have to probe many slots.

####    b. Quadratic Probing
    * **Concept:** If a collision occurs at index `h(key)`, we try slots `(h(key) + 1^2) % m`, then `(h(key) + 2^2) % m`, then `(h(key) + 3^2) % m`, and so on. The interval between probes increases quadratically.
    * **Probe Sequence:** `hash(key), (hash(key) + 1^2) % m, (hash(key) + 2^2) % m, (hash(key) + i^2) % m, ...`
    * **Advantages:**
        * Reduces primary clustering compared to linear probing.
    * **Disadvantages:**
        * **Secondary Clustering:** If two keys have the same initial hash value, their probe sequences will be the same. This is a milder form of clustering.
        * May not probe all locations in the table if the table size `m` is not chosen carefully (e.g., if `m` is a prime number of the form `4k + 3`, quadratic probing will visit all slots).
        * Can be computationally more intensive due to the squaring.

####    c. Double Hashing
    * **Concept:** Uses a second hash function, `h2(key)`, to determine the step size for probing when a collision occurs.
    * **Probe Sequence:** `(h1(key) + i * h2(key)) % m`, where `i = 0, 1, 2, ...`
        * `h1(key)` is the primary hash function.
        * `h2(key)` is a secondary hash function. `h2(key)` must never evaluate to 0. A common choice is `h2(key) = P - (key % P)` where `P` is a prime smaller than `m`.
    * **Advantages:**
        * Significantly reduces both primary and secondary clustering.
        * Distributes keys more evenly than linear or quadratic probing.
        * Often provides the best performance among open addressing techniques in terms of minimizing collisions' impact.
    * **Disadvantages:**
        * More complex to implement due to the need for a second hash function.
        * Computationally more expensive than linear probing due to the second hash calculation per probe.

### 2. Separate Chaining (Open Hashing)

* **Concept:** Each slot in the hash table does not store the element itself, but rather a pointer to a linked list (or other data structure like a binary search tree) of all elements that hash to that slot.
* **How it works:**
    * When a key is hashed to an index, it is inserted into the linked list at that index.
    * If a collision occurs (another key hashes to the same index), the new key is added to the existing linked list.
* **Advantages:**
    * Simple to implement and understand.
    * Hash table never "fills up" in the traditional sense (it can always store more elements, though performance degrades as lists grow).
    * Collision handling is straightforward.
    * Performance degrades more gracefully than open addressing as the load factor increases.
* **Disadvantages:**
    * Uses extra memory for pointers in the linked lists.
    * Can have poor cache performance because elements in a list might be scattered in memory.
    * In the worst case (all keys hash to the same slot), search time becomes O(n) where n is the number of keys.

## Bucket Hashing

* **Concept:** A variation of separate chaining (or sometimes considered a hybrid approach). Instead of a linked list of individual elements at each hash table slot, each slot (called a bucket) can store a fixed number `b` of elements directly.
* **How it works:**
    * When a key hashes to a bucket:
        * If the bucket has empty space, the element is placed there.
        * If the bucket is full, an **overflow handling mechanism** is needed. This could be:
            * Using an overflow bucket (another bucket linked to the primary bucket).
            * Applying an open addressing technique within or beyond the bucket.
            * Using a linked list attached to the bucket for overflow elements (making it very similar to separate chaining but with an initial fixed-size block).
* **Advantages:**
    * Can be more space-efficient than separate chaining if the number of elements per bucket is small and collisions are not too frequent, as it reduces pointer overhead.
    * Can offer good performance if most elements fit within their primary buckets.
* **Disadvantages:**
    * Fixed bucket size can lead to wasted space if buckets are underutilized.
    * If buckets overflow frequently, the performance can degrade, and the complexity of handling overflows increases.
    * Choosing an appropriate bucket size is crucial.

## Deletion

Deleting elements from a hash table requires special consideration, especially in open addressing schemes.

* **Separate Chaining:** Deletion is straightforward. Hash to the correct slot, then delete the element from the linked list (or other data structure) at that slot.
* **Open Addressing:**
    * **Problem:** Simply marking a slot as "empty" after deleting an element can break probe sequences for other elements that collided with the deleted item and were placed further down the probe path. A subsequent search for such an element might incorrectly conclude it's not present when it hits the newly emptied slot.
    * **Solution:** Instead of marking the slot as empty, mark it with a special "deleted" or "tombstone" status.
        * **During Search:** Treat "deleted" slots as occupied when probing for an element, but as empty if an insertion is being performed (an element can be inserted into a "deleted" slot).
    * **Disadvantages of Tombstones:**
        * The table can fill up with "deleted" markers, increasing search times as probes have to skip over them.
        * Requires more complex logic for insertion and search.

## Rehashing

Rehashing is the process of creating a new, larger hash table and then re-inserting all the existing elements from the old table into the new table.

**When is Rehashing Needed?**

1.  **High Load Factor:** The **load factor ($\alpha$)** of a hash table is defined as:
    * $\alpha = \frac{\text{Number of elements}}{\text{Table size}}$
    * As the load factor increases, the probability of collisions increases, and the performance of the hash table (especially for open addressing) degrades significantly.
    * Rehashing is typically done when the load factor exceeds a certain threshold (e.g., 0.5 for quadratic probing/double hashing, or 0.7-0.8 for separate chaining).

2.  **Too Many Deleted Markers (in Open Addressing):** If many elements have been deleted and replaced by tombstones, search times can degrade even if the actual number of elements (and thus the load factor) is low. Rehashing can re-insert only the active elements into a clean, new table.

**Process of Rehashing:**

1.  **Create a new hash table:** Usually, the new table size is approximately double the old table size (and preferably a prime number).
2.  **Iterate through the old table:** For each element in the old table:
    * Recalculate its hash value using the hash function with the **new table size**.
    * Insert the element into the appropriate slot in the **new table**, resolving any collisions in the new table as usual.
3.  **Deallocate the old table:** Once all elements are moved, the memory for the old table can be freed.

**Advantages of Rehashing:**
* Maintains good performance by keeping the load factor low.
* Clears out "deleted" markers in open addressing schemes.

**Disadvantages of Rehashing:**
* It is an expensive operation. It takes O(n) time, where n is the number of elements, because every element needs to be rehashed and reinserted.
* During the rehashing process, operations on the hash table might be slow or temporarily suspended. For real-time systems, strategies like incremental rehashing (rehashing small portions of the table at a time) can be used.

__________________________

# Hashing Notes

## 1. Hash Function

A **Hash Function** is a function that takes an input (or 'key') of arbitrary size and returns a fixed-size string of bytes. This output is typically a numerical value and is called a hash value, hash code, digest, or simply hash.

**Key Properties of a Good Hash Function:**

* **Deterministic:** The same input key must always produce the same hash value.
* **Uniform Distribution:** It should distribute keys as evenly as possible across the hash table to minimize collisions. A good hash function maps keys to seemingly random locations.
* **Efficiency:** It should be computationally easy and fast to compute the hash value for any given key.
* **Avalanche Effect (for cryptographic hashes):** A small change in the input key should produce a significantly different hash value. This is more critical for cryptographic applications than for typical hash table usage.
* **Minimize Collisions:** While collisions are often unavoidable (especially with a large number of keys and a smaller table size), the hash function should aim to reduce their frequency.

## 2. Address Calculation Technique

The core idea of hashing is to use the hash value generated by the hash function to determine the **address** (or index) where the corresponding data record should be stored or retrieved in a data structure called a **hash table** (essentially an array).

**The Process:**

1.  **Key Input:** You have a key (e.g., a username, product ID, or any piece of data you want to store/retrieve quickly).
2.  **Hash Computation:** The key is passed to a hash function.
3.  **Hash Value Generation:** The hash function produces a hash value (usually an integer).
4.  **Address Mapping:** This hash value is then mapped to an index in the hash table. A common way to do this is using the modulo operator:
    `address = hash_value % table_size`
    where `table_size` is the number of slots in the hash table.

This calculated `address` is the "home" position for the key in the hash table.

## 3. Common Hashing Functions

Several methods are used to create hash functions. The choice often depends on the type of keys and the desired properties.

* **Division Method:**
    * **Formula:** `h(key) = key % table_size`
    * **Explanation:** The key (which must be an integer or convertible to one) is divided by the size of the hash table, and the remainder is taken as the hash value (index).
    * **Pros:** Simple and fast.
    * **Cons:**
        * The choice of `table_size` is crucial. If `table_size` is a power of 2 (e.g., $2^p$), then `h(key)` just becomes the lowest `p` bits of `key`, which might not distribute keys well if the lower bits have patterns.
        * It's generally recommended to choose `table_size` as a prime number not too close to a power of 2 to ensure better distribution.

* **Mid-Square Method:**
    * **Explanation:** The key is squared. Then, some digits from the middle of the squared result are extracted and used as the hash value. The number of digits extracted depends on the required table size.
    * **Example:** If key = 93 and table size requires 2 digits:
        1.  Square the key: $93^2 = 8649$.
        2.  Extract middle digits (e.g., 64): `h(93) = 64`.
    * **Pros:** Tends to produce reasonably well-distributed hash values because all digits of the key contribute to the result.
    * **Cons:** Can be computationally more intensive than the division method. The choice of which "middle" digits to take needs careful consideration.

* **Folding Method:**
    * **Explanation:** The key (often a long string or number) is divided into several parts (chunks). These parts are then combined (e.g., added or XORed together) to form the hash value.
    * **Types:**
        * **Fold-shifting:** The chunks are simply added together.
        * **Fold-boundary:** The alternate chunks are reversed before addition.
    * **Example (Fold-shifting for a key 12345678 and a 2-digit hash):**
        1.  Divide into parts: 12, 34, 56, 78.
        2.  Add them: $12 + 34 + 56 + 78 = 180$.
        3.  If the result exceeds the desired hash size (e.g., we need a 2-digit hash), it can be truncated or another method (like modulo) can be applied (e.g., $180 \text{ mod } 100 = 80$).
    * **Pros:** Useful for keys that are longer than the word size of the computer. Ensures that all parts of the key contribute to the hash.
    * **Cons:** The way parts are combined can affect distribution.

* **String Hashing (Example: Polynomial Rolling Hash):**
    * **Explanation:** For string keys, each character is treated as a number (e.g., its ASCII value). A polynomial is formed where each character's value is a coefficient, and it's evaluated modulo `table_size`.
    * **Formula:** `hash(s) = (s[0]*p^(n-1) + s[1]*p^(n-2) + ... + s[n-1]*p^0) % table_size`
        where `s[i]` is the numeric value of the character at index `i`, `p` is a prime number (e.g., 31, 53), and `n` is the length of the string.
    * **Pros:** Good distribution for string keys.
    * **Cons:** Can be more computationally intensive for very long strings.

* **Universal Hashing:**
    * **Explanation:** Instead of using a single, fixed hash function, universal hashing involves selecting a hash function at random from a carefully designed family of hash functions.
    * **Pros:** Provides good average-case performance and makes it difficult for an adversary to choose keys that all hash to the same slot (thus avoiding worst-case scenarios for specific hash functions).
    * **Cons:** Adds a layer of complexity in choosing and managing the family of functions.

## 4. Collision

A **collision** occurs when two or more different keys, after being processed by a hash function, result in the same hash value (i.e., they map to the same slot or index in the hash table).

Since the number of possible keys is often much larger than the number of available slots in the hash table (the pigeonhole principle), collisions are generally unavoidable unless a perfect hash function is used (which is only feasible for a fixed set of known keys).

**Why are collisions problematic?**

If collisions are not handled, trying to insert a new key that collides with an existing key might overwrite the existing data, or it might be impossible to store the new key. Similarly, when searching, a collision might lead to retrieving the wrong data.

Therefore, **collision resolution techniques** are essential for the proper functioning of hash tables.

## 5. Collision Resolution Techniques

When a collision occurs, a strategy is needed to find an alternative location for the new key or to store multiple keys that map to the same initial slot. There are two main approaches:

1.  **Open Addressing (Closed Hashing):** All elements are stored directly within the hash table itself. When a collision occurs, the algorithm probes the table for the next available empty slot according to a predefined sequence.
2.  **Separate Chaining (Open Hashing):** Each slot in the hash table does not hold a single element, but rather a pointer to a data structure (commonly a linked list, but could also be a tree or another hash table) that stores all keys that hashed to that particular slot.

### 5.1. Open Addressing Methods:

In open addressing, if the initial slot `h(key)` is occupied, subsequent slots are probed until an empty slot is found. The probe sequence is defined by:

`h_i(key) = (h(key) + f(i)) % table_size`

where `h(key)` is the initial hash, `f(i)` is the collision resolution function with `i` being the probe attempt number (starting from $i=0, 1, 2, ...$).

#### a. Linear Probing

* **How it works:** If a collision occurs at index `h(key)`, it checks the next slot `(h(key) + 1) % table_size`. If that is also occupied, it checks `(h(key) + 2) % table_size`, and so on, wrapping around the table if necessary.
* **Formula for `f(i)`:** `f(i) = i`
    So, the probe sequence is: `h(key)`, `(h(key) + 1) % M`, `(h(key) + 2) % M`, ... (where M is table_size)
* **Advantages:**
    * Simple to implement.
    * Good cache performance due to sequential memory access (locality of reference).
* **Disadvantages:**
    * **Primary Clustering:** Keys that hash to nearby slots tend to form long runs of occupied slots. This means that if a key hashes into a cluster, the probe sequence to find an empty slot (or to find the key during search/deletion) can become very long, degrading performance towards $O(N)$ in the worst case.
    * Deletion is complex: Simply marking a slot as "empty" can break the probe sequence for other keys. A special "deleted" marker is often used.

#### b. Quadratic Probing

* **How it works:** The interval between probes increases quadratically. If `h(key)` is occupied, subsequent probes are at `(h(key) + 1^2) % M`, `(h(key) + 2^2) % M`, `(h(key) + 3^2) % M`, and so on.
* **Formula for `f(i)`:** `f(i) = c1*i + c2*i^2` (A common form is `f(i) = i^2`).
    So, the probe sequence is: `h(key)`, `(h(key) + 1) % M`, `(h(key) + 4) % M`, `(h(key) + 9) % M`, ...
* **Advantages:**
    * Reduces primary clustering significantly compared to linear probing. Keys that hash to the same initial location will follow the same probe sequence, but keys that hash to different locations are less likely to form contiguous clusters.
* **Disadvantages:**
    * **Secondary Clustering:** If multiple keys hash to the same initial slot, they will all follow the same sequence of probed slots. This is less severe than primary clustering.
    * May not probe all slots in the table if the table size and quadratic function are not chosen carefully. For example, if `table_size` is a prime number and `f(i) = i^2`, it's guaranteed to find an empty slot if the table is less than half full.
    * Deletion is also complex, requiring a "deleted" marker.

#### c. Double Hashing

* **How it works:** Uses a second, independent hash function `h2(key)` to determine the step size for the probe sequence. If `h(key)` (from the first hash function `h1(key)`) is occupied, the next probe is at `(h1(key) + 1*h2(key)) % M`, then `(h1(key) + 2*h2(key)) % M`, and so on.
* **Formula for `f(i)`:** `f(i) = i * h2(key)`
    So, the probe sequence is: `h1(key)`, `(h1(key) + h2(key)) % M`, `(h1(key) + 2*h2(key)) % M`, ...
* **Properties of `h2(key)`:**
    * `h2(key)` should never evaluate to 0 (to avoid an infinite loop on the initial slot).
    * `h2(key)` should be relatively prime to `table_size` (ideally `table_size` is prime, and `1 <= h2(key) < table_size`) to ensure that all slots can be probed. A common choice is `h2(key) = R - (key % R)` where `R` is a prime smaller than `table_size`.
* **Advantages:**
    * Significantly reduces both primary and secondary clustering. The probe sequence depends on the key itself, so different keys that initially collide are likely to have different probe sequences.
    * Distributes keys more uniformly than linear or quadratic probing.
* **Disadvantages:**
    * More computationally expensive due to the calculation of a second hash function.
    * Poorer cache performance compared to linear probing as memory accesses are more scattered.
    * Deletion is complex.

### 5.2. Separate Chaining

* **How it works:** Each slot `M[i]` in the hash table acts as a head of a linked list (or other collection). All keys that hash to index `i` are stored in the linked list at `M[i]`.
* **Insertion:**
    1.  Compute `index = h(key)`.
    2.  Insert the key (and its value) into the linked list at `table[index]`. This can be done at the beginning, end, or in sorted order within the list.
* **Search:**
    1.  Compute `index = h(key)`.
    2.  Traverse the linked list at `table[index]` to find the key.
* **Deletion:**
    1.  Compute `index = h(key)`.
    2.  Search for the key in the linked list at `table[index]` and remove it.
* **Advantages:**
    * Simple to implement.
    * Collisions are handled naturally by adding to the list.
    * The hash table itself never "fills up" in the same way as open addressing (though performance degrades as lists get longer). The load factor can exceed 1.
    * Deletion is straightforward (standard linked list deletion).
* **Disadvantages:**
    * Requires extra memory for pointers in the linked lists.
    * Can have poor cache performance if lists become long, as elements in a list might not be contiguous in memory.
    * In the worst case (all keys hash to the same slot), performance degrades to $O(N)$ for search, insert, and delete as it becomes a linear search through one long list.

## 6. Bucket Hashing (Variant of Separate Chaining or Open Addressing)

Bucket Hashing is a technique where each slot in the hash table (often called a bucket) can hold multiple records.

* **How it works:**
    1.  The hash function `h(key)` maps a key to a specific bucket.
    2.  Instead of a single slot, each bucket has a capacity to store a fixed number (`B`) of records.
    3.  **Insertion:** When a key is hashed to a bucket:
        * If the bucket has an empty slot, the key is placed there.
        * If the bucket is full, a collision resolution strategy is needed. This can involve:
            * **Overflow Buckets:** An overflow bucket (often shared or a linked list of overflow buckets) is used to store records that don't fit in their primary bucket. This is similar to separate chaining where the bucket itself is a small array and overflows go to a linked list.
            * **Probing (within open addressing context):** If buckets are part of an open addressing scheme, a full bucket might trigger probing for the next available slot in the *next bucket* or according to a probing sequence that considers buckets.
* **Structure:** The hash table is an array of buckets, and each bucket is itself typically a small array or list of fixed size.
* **Advantages:**
    * Can be more space-efficient than separate chaining if the number of items per bucket is small and fixed, as it avoids pointer overhead for every element.
    * Can improve cache performance compared to separate chaining with long lists, as elements within a bucket are stored contiguously.
    * Reduces the probability of collisions requiring overflow handling compared to a simple hash table where each slot holds only one item, assuming a good distribution into buckets.
* **Disadvantages:**
    * If many keys hash to the same bucket, that bucket can fill up, and performance will depend on how overflows are handled.
    * Fixed bucket size can lead to wasted space if many buckets are sparsely populated.
    * Choosing an appropriate bucket size is important.

## 7. Deletion and Rehashing

### Deletion

Deleting elements from a hash table requires careful consideration, especially in open addressing schemes.

* **Separate Chaining:**
    * Deletion is relatively straightforward.
    1.  Calculate the hash `index = h(key)`.
    2.  Search for the key in the linked list (or other structure) at `table[index]`.
    3.  If found, remove it using the standard deletion mechanism for that data structure (e.g., remove node from linked list).

* **Open Addressing (Linear Probing, Quadratic Probing, Double Hashing):**
    * Simply marking a slot as "empty" after deleting an element can be problematic. If an element `X` was inserted after a collision and its probe sequence passed through a slot `S` that is now marked empty, a subsequent search for `X` might terminate prematurely at `S`, incorrectly concluding `X` is not in the table.
    * **Solution: Lazy Deletion / Tombstones:**
        * Instead of marking a slot as truly empty, it is marked with a special "deleted" or "tombstone" marker.
        * **Search:** During a search, if a "deleted" slot is encountered, the search continues (as the desired key might be further along the probe sequence). The search stops only when the key is found or an actual "empty" (never used) slot is encountered.
        * **Insertion:** A new key can be inserted into either an "empty" slot or a "deleted" slot. Inserting into a "deleted" slot reuses space.
    * **Disadvantage of Tombstones:** Over time, the table can fill up with "deleted" markers, which can increase average search times as probe sequences still have to step over them. This necessitates rehashing eventually.

### Rehashing

Rehashing is the process of creating a new, usually larger, hash table and re-inserting all the existing elements from the old table into the new one.

**When is Rehashing Needed?**

1.  **High Load Factor (Open Addressing & Separate Chaining):**
    * The **load factor ($\alpha$)** is the ratio of the number of elements (`n`) to the number of slots in the hash table (`m`): $\alpha = n/m$.
    * In open addressing, as $\alpha$ approaches 1, the number of collisions increases dramatically, and probe sequences become very long, leading to poor performance ($O(N)$). Rehashing is typically done when $\alpha$ exceeds a certain threshold (e.g., 0.5 for quadratic probing, or 0.7-0.8 for linear probing or chaining with good hash functions).
    * In separate chaining, as $\alpha$ (average list length) increases, search times increase. Rehashing to a larger table can keep the average list lengths short.
2.  **Too Many Tombstones (Open Addressing):** If many deletions have occurred, the table might contain numerous "deleted" markers, degrading search performance even if the actual number of active elements is low. Rehashing effectively removes these tombstones.
3.  **Changing Hash Function Requirements:** Sometimes, the nature of keys might change, or a better hash function is discovered, necessitating a rehash.

**How Rehashing Works:**

1.  **Allocate New Table:** A new hash table is allocated, typically with a size that is roughly double the current size (and often chosen to be a prime number).
2.  **Iterate and Re-insert:** For each element in the old hash table:
    * If it's an active element (not a tombstone in open addressing), its hash value is re-calculated using the hash function with respect to the **new table size**.
    * The element is then inserted into the new table at its new calculated position (handling any collisions in the new table).
3.  **Replace Old Table:** The old hash table is deallocated, and the new table becomes the active hash table.

**Advantages of Rehashing:**

* Maintains good average-case performance ($O(1)$ for search, insert, delete) by keeping the load factor low.
* Eliminates tombstones in open addressing.

**Disadvantages of Rehashing:**

* It is an expensive operation. Re-inserting all `n` elements takes $O(N)$ time (or $O(N+M)$ where M is the new table size), during which other operations on the hash table might be slow or blocked.
* **Amortized Analysis:** While a single rehash operation is costly, if the table size is doubled each time, the cost of rehashing, when averaged over a long sequence of insertions, contributes only a constant factor to the average cost per insertion. This is why hash table operations are often said to have *amortized* $O(1)$ complexity.

**Strategies to Mitigate Rehashing Cost:**

* **Incremental Rehashing:** Instead of rehashing all elements at once, a small number of elements are moved from the old table to the new table during each regular insertion or deletion operation. This spreads the cost over time. Searches might need to check both tables until the migration is complete.
* **Concurrent Rehashing:** In multi-threaded environments, rehashing can be performed by a separate thread to minimize impact on ongoing operations.
